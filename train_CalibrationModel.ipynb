{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "print(gpus)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os,glob, random\n",
    "import tensorflow_models as tfm\n",
    "from ptge import GazeModel, vgg16_processor\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face=[]\n",
    "lefteye=[]\n",
    "righteye=[]\n",
    "rotation_matrix=[]\n",
    "flipped_rotation_matrix=[]\n",
    "gaze=[]\n",
    "gaze_flipped=[]\n",
    "subject_id=[]\n",
    "eye_coords=[]\n",
    "subject_map={}\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "print(persons)\n",
    "id=0\n",
    "for person in persons[:2]:\n",
    "    face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "    lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "    righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "    rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "    flipped_rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix_flipped/*')\n",
    "    gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "    gaze_flipped+=glob.glob(f'{data_path}/{person}/3d_gaze_flipped/*')\n",
    "    subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "    eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "    subject_map[f'{data_path}/{person}']=id\n",
    "    id+=1\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "flipped_rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "gaze_flipped.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,flipped_rotation_matrix,eye_coords,gaze,gaze_flipped,subject_id))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)\n",
    "print(subject_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_map=tf.lookup.StaticHashTable( tf.lookup.KeyValueTensorInitializer(list(subject_map.keys()), \n",
    "                                                                           list(subject_map.values())),default_value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GazeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gaze_model=GazeModel()\n",
    "print(gaze_model({'face':tf.ones((1,224,224,3)),\n",
    "        'flipped_face':tf.ones((1,224,224,3)),\n",
    "        'lefteye':tf.ones((1,36,60,3)),\n",
    "        'righteye':tf.ones((1,36,60,3)),\n",
    "        'rotation_matrix':tf.ones((1,9)),\n",
    "        'eye_coords':tf.ones((1,6)),\n",
    "        'id':tf.constant([1.])}))\n",
    "gaze_model.load_weights('best_GazeModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for tf.data, subject embedding from GazeModel used as ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_img(img):\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.io.decode_jpeg(img,3)\n",
    "    return img\n",
    "@tf.numpy_function(Tout=tf.float32)\n",
    "def ld(x):\n",
    "    return np.load(x).astype('float32').ravel()\n",
    "@tf.function\n",
    "def map_fn(face,\n",
    "            lefteye,\n",
    "            righteye,\n",
    "            rotation_matrix,\n",
    "            flipped_rotation_matrix,\n",
    "            eye_coords,\n",
    "            gaze,\n",
    "            gaze_flipped,\n",
    "            subject_id,\n",
    "            ):\n",
    "    face=load_img(face)\n",
    "    flipped_face=tf.image.flip_left_right(face)\n",
    "    lefteye=load_img(lefteye)\n",
    "    righteye=load_img(righteye)\n",
    "    rotation_matrix=ld(rotation_matrix)\n",
    "    flipped_rotation_matrix=ld(flipped_rotation_matrix)\n",
    "    eye_coords=ld(eye_coords)\n",
    "    id=subject_map[subject_id]\n",
    "    gaze=ld(gaze)\n",
    "    gaze_flipped=ld(gaze_flipped)\n",
    "    subject_embedding=gaze_model.embedding(id)\n",
    "    return {\n",
    "            'face':face,\n",
    "            'flipped_face':flipped_face,\n",
    "            'lefteye':lefteye,\n",
    "            'righteye':righteye,\n",
    "            'rotation_matrix':rotation_matrix,\n",
    "            'flipped_rotation_matrix':flipped_rotation_matrix,\n",
    "            'eye_coords':eye_coords,\n",
    "            'gaze':gaze,\n",
    "            'gaze_flipped':gaze_flipped,\n",
    "            },subject_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack of transformer blocks as described in the paper with 6 blocks each with 4 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = tfm.nlp.models.TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=2048,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0,\n",
    "    attention_dropout_rate=0.0,\n",
    "    use_bias=not False,\n",
    "    norm_first=True,\n",
    "    norm_epsilon=1e-06,\n",
    "    intermediate_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design of calibration model with transformers stacked between MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_model.trainable=False\n",
    "class CalibrationModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CalibrationModel,self).__init__()\n",
    "        #g_face from trained gaze_model\n",
    "        self.g_face=gaze_model.g_face\n",
    "        #g_eye from trained gaze_model\n",
    "        self.g_eye=gaze_model.g_eye\n",
    "        #transformer encoder stack\n",
    "        self.transformer_stack=transformer\n",
    "        self.flat=tf.keras.layers.Flatten()\n",
    "        #pre transformer MLP\n",
    "        self.MLP1=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP1')\n",
    "        #post transformer MLP\n",
    "        self.MLP2=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP2')\n",
    "        #final output layer predicting person specific preference vector of lenght 6\n",
    "        self.output_layer=tf.keras.layers.Dense(6,name='subject_feature')\n",
    "    def call(self,input_dict):\n",
    "        #face features from GazeModel\n",
    "        face_features=self.g_face(input_dict['face'])\n",
    "        #flipped face features from GazeModel( left face right face combination)\n",
    "        flipped_face_features=self.g_face(input_dict['flipped_face'])\n",
    "        #left eye features from gazeModel\n",
    "        left_features=vgg16_processor(input_dict['lefteye'])\n",
    "        left_features=self.g_eye(left_features)\n",
    "        #right eye features from gazeModel\n",
    "        right_features=vgg16_processor(input_dict['righteye'])\n",
    "        right_features=self.g_eye(right_features)\n",
    "        #flatteneded feature matrices\n",
    "        face_features=self.flat(face_features)\n",
    "        flipped_face_features=self.flat(flipped_face_features)\n",
    "        left_features=self.flat(left_features)\n",
    "        right_features=self.flat(right_features)\n",
    "        #face roration matrix\n",
    "        rot_mat=input_dict['rotation_matrix']\n",
    "        #left face rotation matrix\n",
    "        rot_mat_flipped=input_dict['flipped_rotation_matrix']\n",
    "        #3d eye coordinates left eye,right eye\n",
    "        eye_coords=input_dict['eye_coords']\n",
    "        #3d gaze\n",
    "        gaze=input_dict['gaze']\n",
    "        #left gaze flipped (left,right combo)\n",
    "        gaze_flipped=input_dict['gaze_flipped']\n",
    "        #concatenated features\n",
    "        total=tf.concat([face_features,flipped_face_features,left_features,\n",
    "                            right_features,eye_coords,rot_mat,rot_mat_flipped,\n",
    "                            gaze,gaze_flipped],1)\n",
    "        #pre transformer MLP\n",
    "        total=self.MLP1(total)\n",
    "        total = tf.expand_dims(total, axis=1) \n",
    "        total=self.transformer_stack(total)\n",
    "        #post-transformer MLP\n",
    "        total=self.MLP2(tf.squeeze(total,1))\n",
    "        #final output of predicted preference vector\n",
    "        final_output=self.output_layer(total)\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,\n",
    "                                   beta_1=0.9,\n",
    "                                   beta_2=0.999,\n",
    "                                   epsilon=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CalibrationModel()\n",
    "model.compile(loss='MSE',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(next(iter(data.map(map_fn).batch(1).map(lambda x,y:x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split and train stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.take(data.cardinality().numpy()*0.8)\n",
    "test_data=data.skip(data.cardinality().numpy()*0.8)\n",
    "cb=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=10,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data.map(map_fn,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(32,num_parallel_calls=tf.data.AUTOTUNE).prefetch(2),epochs=200,\n",
    "          validation_data=test_data.map(map_fn).batch(100),callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('calibr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for new person p02 for whom calibration has to be done to estimate its embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face=[]\n",
    "lefteye=[]\n",
    "righteye=[]\n",
    "rotation_matrix=[]\n",
    "flipped_rotation_matrix=[]\n",
    "gaze=[]\n",
    "gaze_flipped=[]\n",
    "subject_id=[]\n",
    "eye_coords=[]\n",
    "subject_map={'processed_data/Image/p00': 0, 'processed_data/Image/p01': 1}\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "person=persons[2]\n",
    "face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "flipped_rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix_flipped/*')\n",
    "gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "gaze_flipped+=glob.glob(f'{data_path}/{person}/3d_gaze_flipped/*')\n",
    "subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "subject_map[f'{data_path}/{person}']=2\n",
    "\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "flipped_rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "gaze_flipped.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,flipped_rotation_matrix,eye_coords,gaze,gaze_flipped,subject_id))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)\n",
    "print(subject_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 batches of 16 calibration samples taken for estimating person specific embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=data.take(16*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=128>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(next(iter(new_data.map(map_fn).batch(16).map(lambda x,y:x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batches of 16 calibration samples are forward passed through the calibration model every batch predicts 16 preference vectors but we want same embeddings for same person so angular difference between these vectors should be minimum so cosine similarity is taken as cumulutive loss as with . tf cosine similarity gives -1 as perfect match so the best loss will be -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop for sample calibration for new person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss=999\n",
    "for e in range(100):\n",
    "    losses=[]\n",
    "    for data in new_data.map(map_fn).batch(16).map(lambda x,y:x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(data, training=True)\n",
    "            loss=tf.reduce_sum(tf.keras.losses.cosine_similarity(logits[0],logits[1:]))\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        # tf.print(loss)\n",
    "        losses.append(loss)\n",
    "    mean_loss=tf.reduce_mean(losses)\n",
    "    if mean_loss<best_loss:\n",
    "        model.save_weights('calibr.h5')\n",
    "        tf.print(mean_loss)\n",
    "        best_loss=mean_loss.numpy()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
