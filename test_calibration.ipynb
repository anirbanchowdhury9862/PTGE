{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "print(gpus)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os,glob, random\n",
    "import tensorflow_models as tfm\n",
    "from ptge import GazeModel, vgg16_processor\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load gaze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 12:52:44.132776: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.59598786  0.85539657 -0.2608078 ]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gaze_model=GazeModel()\n",
    "print(gaze_model({'face':tf.ones((1,224,224,3)),\n",
    "        'flipped_face':tf.ones((1,224,224,3)),\n",
    "        'lefteye':tf.ones((1,36,60,3)),\n",
    "        'righteye':tf.ones((1,36,60,3)),\n",
    "        'rotation_matrix':tf.ones((1,9)),\n",
    "        'eye_coords':tf.ones((1,6)),\n",
    "        'id':tf.constant([1.])}))\n",
    "gaze_model.load_weights('best_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = tfm.nlp.models.TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=2048,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0,\n",
    "    attention_dropout_rate=0.0,\n",
    "    use_bias=not False,\n",
    "    norm_first=True,\n",
    "    norm_epsilon=1e-06,\n",
    "    intermediate_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_model.trainable=False\n",
    "class CalibrationModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CalibrationModel,self).__init__()\n",
    "        self.g_face=gaze_model.g_face\n",
    "        self.g_eye=gaze_model.g_eye\n",
    "        self.transformer_stack=transformer\n",
    "        self.flat=tf.keras.layers.Flatten()\n",
    "        self.MLP1=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP1')\n",
    "        self.MLP2=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP2')\n",
    "        self.output_layer=tf.keras.layers.Dense(6,name='subject_feature')\n",
    "    def call(self,input_dict):\n",
    "        face_features=self.g_face(input_dict['face'])\n",
    "        flipped_face_features=self.g_face(input_dict['flipped_face'])\n",
    "        left_features=vgg16_processor(input_dict['lefteye'])\n",
    "        left_features=self.g_eye(left_features)\n",
    "        right_features=vgg16_processor(input_dict['righteye'])\n",
    "        right_features=self.g_eye(right_features)\n",
    "        face_features=self.flat(face_features)\n",
    "        flipped_face_features=self.flat(flipped_face_features)\n",
    "        left_features=self.flat(left_features)\n",
    "        right_features=self.flat(right_features)\n",
    "        rot_mat=input_dict['rotation_matrix']\n",
    "        rot_mat_flipped=input_dict['flipped_rotation_matrix']\n",
    "        eye_coords=input_dict['eye_coords']\n",
    "        gaze=input_dict['gaze']\n",
    "        gaze_flipped=input_dict['gaze_flipped']\n",
    "        total=tf.concat([face_features,flipped_face_features,left_features,\n",
    "                            right_features,eye_coords,rot_mat,rot_mat_flipped,\n",
    "                            gaze,gaze_flipped],1)\n",
    "        total=self.MLP1(total)\n",
    "        total = tf.expand_dims(total, axis=1) \n",
    "        total=self.transformer_stack(total)\n",
    "        total=self.MLP2(tf.squeeze(total,1))\n",
    "        final_output=self.output_layer(total)\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for newperson to estimate person speciific preference vector using calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_data/Image/p00': 0, 'processed_data/Image/p01': 1, 'processed_data/Image/p02': 2}\n"
     ]
    }
   ],
   "source": [
    "face=[]\n",
    "lefteye=[]\n",
    "righteye=[]\n",
    "rotation_matrix=[]\n",
    "flipped_rotation_matrix=[]\n",
    "gaze=[]\n",
    "gaze_flipped=[]\n",
    "subject_id=[]\n",
    "eye_coords=[]\n",
    "subject_map={'processed_data/Image/p00': 0, 'processed_data/Image/p01': 1}\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "person=persons[2]\n",
    "face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "flipped_rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix_flipped/*')\n",
    "gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "gaze_flipped+=glob.glob(f'{data_path}/{person}/3d_gaze_flipped/*')\n",
    "subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "subject_map[f'{data_path}/{person}']=2\n",
    "\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "flipped_rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "gaze_flipped.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,flipped_rotation_matrix,eye_coords,gaze,gaze_flipped,subject_id))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)\n",
    "print(subject_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_map=tf.lookup.StaticHashTable( tf.lookup.KeyValueTensorInitializer(list(subject_map.keys()), \n",
    "                                                                           list(subject_map.values())),default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_img(img):\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.io.decode_jpeg(img,3)\n",
    "    return img\n",
    "@tf.numpy_function(Tout=tf.float32)\n",
    "def ld(x):\n",
    "    return np.load(x).astype('float32').ravel()\n",
    "@tf.function\n",
    "def map_fn_calibration(face,\n",
    "            lefteye,\n",
    "            righteye,\n",
    "            rotation_matrix,\n",
    "            flipped_rotation_matrix,\n",
    "            eye_coords,\n",
    "            gaze,\n",
    "            gaze_flipped,\n",
    "            subject_id,\n",
    "            ):\n",
    "    face=load_img(face)\n",
    "    flipped_face=tf.image.flip_left_right(face)\n",
    "    lefteye=load_img(lefteye)\n",
    "    righteye=load_img(righteye)\n",
    "    rotation_matrix=ld(rotation_matrix)\n",
    "    flipped_rotation_matrix=ld(flipped_rotation_matrix)\n",
    "    eye_coords=ld(eye_coords)\n",
    "    id=subject_map[subject_id]\n",
    "    gaze=ld(gaze)\n",
    "    gaze_flipped=ld(gaze_flipped)\n",
    "    subject_embedding=gaze_model.embedding(id)\n",
    "    return {\n",
    "            'face':face,\n",
    "            'flipped_face':flipped_face,\n",
    "            'lefteye':lefteye,\n",
    "            'righteye':righteye,\n",
    "            'rotation_matrix':rotation_matrix,\n",
    "            'flipped_rotation_matrix':flipped_rotation_matrix,\n",
    "            'eye_coords':eye_coords,\n",
    "            'gaze':gaze,\n",
    "            'gaze_flipped':gaze_flipped,\n",
    "            }\n",
    "@tf.function\n",
    "def map_fn_gaze(face,lefteye,righteye,rotation_matrix,eye_coords,subject_id,gaze):\n",
    "    face=load_img(face)\n",
    "    flipped_face=tf.image.flip_left_right(face)\n",
    "    lefteye=load_img(lefteye)\n",
    "    righteye=load_img(righteye)\n",
    "    rotation_matrix=ld(rotation_matrix)\n",
    "    eye_coords=ld(eye_coords)\n",
    "    id=subject_map[subject_id]\n",
    "    gaze=ld(gaze)\n",
    "    return {\n",
    "            'face':face,\n",
    "            'flipped_face':flipped_face,\n",
    "            'lefteye':lefteye,\n",
    "            'righteye':righteye,\n",
    "            'rotation_matrix':rotation_matrix,\n",
    "            'eye_coords':eye_coords,\n",
    "            'id':id},gaze\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CalibrationModel()\n",
    "model(next(iter(data.map(map_fn_calibration).batch(1))))\n",
    "model.load_weights('calibr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation of person specific embedding for new person p02 using calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[-0.24915332,  0.6478733 , -1.0298878 , -0.3768737 ,  0.36539593,\n",
       "        -0.5966729 ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings=model(next(iter(data.map(map_fn_calibration).batch(1))))\n",
    "new_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updating the person specific embedding table of the gazeModel with the embeddings of new person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=gaze_model.embedding.weights[0].numpy()\n",
    "mat=np.append(mat,new_embeddings,axis=0)\n",
    "gaze_model.embedding=tf.keras.layers.Embedding(3,6,embeddings_initializer=tf.keras.initializers.Constant(mat),\n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(l2=0.01),\n",
    "                                            mask_zero=True,name='subject_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "array([-0.24915332,  0.6478733 , -1.0298878 , -0.3768737 ,  0.36539593,\n",
       "       -0.5966729 ], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model.embedding(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngularError(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='mean_angular_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_error = self.add_weight(name='total_error', initializer='zeros')\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred,sample_weight=None):\n",
    "        y_true = tf.math.l2_normalize(y_true, axis=-1)\n",
    "        y_pred = tf.math.l2_normalize(y_pred, axis=-1)\n",
    "    \n",
    "        dot_product = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        dot_product = tf.clip_by_value(dot_product, -1.0, 1.0)\n",
    "     \n",
    "        angular_error = tf.acos(dot_product)\n",
    "        angular_error=angular_error*57.296\n",
    "        self.total_error.assign_add(tf.reduce_sum(angular_error))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_error / self.num_samples\n",
    "    def reset_state(self):\n",
    "        self.total_error.assign(0.0)\n",
    "        self.num_samples.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data of new person to be tested using the gazemodel if the calibration is done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face,lefteye,righteye,rotation_matrix,gaze,subject_id,eye_coords=[],[],[],[],[],[],[]\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "print(persons)\n",
    "person=persons[0]\n",
    "face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,eye_coords,subject_id,gaze))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.02049856,  0.10625822, -0.9727191 ]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model(next(iter(data.map(map_fn_gaze).batch(1).map(lambda x,y:x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking evaluation metrics for the new person using the updated gazemodel after calibrating the embeddings using calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,labels=[],[]\n",
    "for dat,label in data.map(map_fn_gaze).batch(1000):\n",
    "    preds+=gaze_model(dat).numpy().tolist()\n",
    "    labels+=label.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.869247>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er=AngularError()\n",
    "er.update_state(labels,preds)\n",
    "er.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
