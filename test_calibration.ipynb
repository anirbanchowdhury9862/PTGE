{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:41:21.639334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-23 17:41:21.661787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-23 17:41:21.661806: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-23 17:41:21.662750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-23 17:41:21.666750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:41:22.843034: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:22.846807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:22.846912: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:41:23.691752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.691892: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.691979: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.749855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.749966: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.750058: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-23 17:41:23.750137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22272 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "print(gpus)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os,glob, random\n",
    "import tensorflow_models as tfm\n",
    "from ptge import GazeModel, vgg16_processor\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load gaze model from trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 17:41:30.827723: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.7601718   0.06174529 -0.2708553 ]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gaze_model=GazeModel()\n",
    "print(gaze_model({'face':tf.ones((1,224,224,3)),\n",
    "        'flipped_face':tf.ones((1,224,224,3)),\n",
    "        'lefteye':tf.ones((1,36,60,3)),\n",
    "        'righteye':tf.ones((1,36,60,3)),\n",
    "        'rotation_matrix':tf.ones((1,9)),\n",
    "        'eye_coords':tf.ones((1,6)),\n",
    "        'id':tf.constant([1.])}))\n",
    "gaze_model.load_weights('best_GazeModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = tfm.nlp.models.TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=2048,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0,\n",
    "    attention_dropout_rate=0.0,\n",
    "    use_bias=not False,\n",
    "    norm_first=True,\n",
    "    norm_epsilon=1e-06,\n",
    "    intermediate_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_model.trainable=False\n",
    "class CalibrationModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CalibrationModel,self).__init__()\n",
    "        self.g_face=gaze_model.g_face\n",
    "        self.g_eye=gaze_model.g_eye\n",
    "        self.transformer_stack=transformer\n",
    "        self.flat=tf.keras.layers.Flatten()\n",
    "        self.MLP1=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP1')\n",
    "        self.MLP2=tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1280,activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            ],name='MLP2')\n",
    "        self.output_layer=tf.keras.layers.Dense(6,name='subject_feature')\n",
    "    def call(self,input_dict):\n",
    "        face_features=self.g_face(input_dict['face'])\n",
    "        flipped_face_features=self.g_face(input_dict['flipped_face'])\n",
    "        left_features=vgg16_processor(input_dict['lefteye'])\n",
    "        left_features=self.g_eye(left_features)\n",
    "        right_features=vgg16_processor(input_dict['righteye'])\n",
    "        right_features=self.g_eye(right_features)\n",
    "        face_features=self.flat(face_features)\n",
    "        flipped_face_features=self.flat(flipped_face_features)\n",
    "        left_features=self.flat(left_features)\n",
    "        right_features=self.flat(right_features)\n",
    "        rot_mat=input_dict['rotation_matrix']\n",
    "        rot_mat_flipped=input_dict['flipped_rotation_matrix']\n",
    "        eye_coords=input_dict['eye_coords']\n",
    "        gaze=input_dict['gaze']\n",
    "        gaze_flipped=input_dict['gaze_flipped']\n",
    "        total=tf.concat([face_features,flipped_face_features,left_features,\n",
    "                            right_features,eye_coords,rot_mat,rot_mat_flipped,\n",
    "                            gaze,gaze_flipped],1)\n",
    "        total=self.MLP1(total)\n",
    "        total = tf.expand_dims(total, axis=1) \n",
    "        total=self.transformer_stack(total)\n",
    "        total=self.MLP2(tf.squeeze(total,1))\n",
    "        final_output=self.output_layer(total)\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for newperson p02 to estimate person speciific preference vector using calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_data/Image/p00': 0, 'processed_data/Image/p01': 1, 'processed_data/Image/p02': 2}\n"
     ]
    }
   ],
   "source": [
    "face=[]\n",
    "lefteye=[]\n",
    "righteye=[]\n",
    "rotation_matrix=[]\n",
    "flipped_rotation_matrix=[]\n",
    "gaze=[]\n",
    "gaze_flipped=[]\n",
    "subject_id=[]\n",
    "eye_coords=[]\n",
    "subject_map={'processed_data/Image/p00': 0, 'processed_data/Image/p01': 1}\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "person=persons[2]\n",
    "face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "flipped_rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix_flipped/*')\n",
    "gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "gaze_flipped+=glob.glob(f'{data_path}/{person}/3d_gaze_flipped/*')\n",
    "subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "subject_map[f'{data_path}/{person}']=2\n",
    "\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "flipped_rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "gaze_flipped.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,flipped_rotation_matrix,eye_coords,gaze,gaze_flipped,subject_id))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)\n",
    "print(subject_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_map=tf.lookup.StaticHashTable( tf.lookup.KeyValueTensorInitializer(list(subject_map.keys()), \n",
    "                                                                           list(subject_map.values())),default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_img(img):\n",
    "    img=tf.io.read_file(img)\n",
    "    img=tf.io.decode_jpeg(img,3)\n",
    "    return img\n",
    "@tf.numpy_function(Tout=tf.float32)\n",
    "def ld(x):\n",
    "    return np.load(x).astype('float32').ravel()\n",
    "@tf.function\n",
    "def map_fn_calibration(face,\n",
    "            lefteye,\n",
    "            righteye,\n",
    "            rotation_matrix,\n",
    "            flipped_rotation_matrix,\n",
    "            eye_coords,\n",
    "            gaze,\n",
    "            gaze_flipped,\n",
    "            subject_id,\n",
    "            ):\n",
    "    face=load_img(face)\n",
    "    flipped_face=tf.image.flip_left_right(face)\n",
    "    lefteye=load_img(lefteye)\n",
    "    righteye=load_img(righteye)\n",
    "    rotation_matrix=ld(rotation_matrix)\n",
    "    flipped_rotation_matrix=ld(flipped_rotation_matrix)\n",
    "    eye_coords=ld(eye_coords)\n",
    "    id=subject_map[subject_id]\n",
    "    gaze=ld(gaze)\n",
    "    gaze_flipped=ld(gaze_flipped)\n",
    "    subject_embedding=gaze_model.embedding(id)\n",
    "    return {\n",
    "            'face':face,\n",
    "            'flipped_face':flipped_face,\n",
    "            'lefteye':lefteye,\n",
    "            'righteye':righteye,\n",
    "            'rotation_matrix':rotation_matrix,\n",
    "            'flipped_rotation_matrix':flipped_rotation_matrix,\n",
    "            'eye_coords':eye_coords,\n",
    "            'gaze':gaze,\n",
    "            'gaze_flipped':gaze_flipped,\n",
    "            }\n",
    "@tf.function\n",
    "def map_fn_gaze(face,lefteye,righteye,rotation_matrix,eye_coords,subject_id,gaze):\n",
    "    face=load_img(face)\n",
    "    flipped_face=tf.image.flip_left_right(face)\n",
    "    lefteye=load_img(lefteye)\n",
    "    righteye=load_img(righteye)\n",
    "    rotation_matrix=ld(rotation_matrix)\n",
    "    eye_coords=ld(eye_coords)\n",
    "    id=subject_map[subject_id]\n",
    "    gaze=ld(gaze)\n",
    "    return {\n",
    "            'face':face,\n",
    "            'flipped_face':flipped_face,\n",
    "            'lefteye':lefteye,\n",
    "            'righteye':righteye,\n",
    "            'rotation_matrix':rotation_matrix,\n",
    "            'eye_coords':eye_coords,\n",
    "            'id':id},gaze\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibration model loading from pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CalibrationModel()\n",
    "model(next(iter(data.map(map_fn_calibration).batch(1))))\n",
    "model.load_weights('calibr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation of person specific embedding for new person p02 using calibration model while the gaze model is not trained on p02 data it is trained only on data of p00 and p01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[-0.5606081 ,  0.17779258,  0.47259313,  0.03219127, -0.58562124,\n",
       "         0.21841398]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings=model(next(iter(data.map(map_fn_calibration).batch(1))))\n",
    "new_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updating the person specific embedding table of the gazeModel with the embeddings of new person p02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "mat=gaze_model.embedding.weights[0].numpy()\n",
    "mat[2]=new_embeddings\n",
    "print(mat.shape)\n",
    "gaze_model.embedding=tf.keras.layers.Embedding(3,6,embeddings_initializer=tf.keras.initializers.Constant(mat),\n",
    "                                            embeddings_regularizer=tf.keras.regularizers.L2(l2=0.01),\n",
    "                                            mask_zero=True,name='subject_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "array([-0.5606081 ,  0.17779258,  0.47259313,  0.03219127, -0.58562124,\n",
       "        0.21841398], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model.embedding(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngularError(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='mean_angular_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_error = self.add_weight(name='total_error', initializer='zeros')\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred,sample_weight=None):\n",
    "        y_true = tf.math.l2_normalize(y_true, axis=-1)\n",
    "        y_pred = tf.math.l2_normalize(y_pred, axis=-1)\n",
    "    \n",
    "        dot_product = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        dot_product = tf.clip_by_value(dot_product, -1.0, 1.0)\n",
    "     \n",
    "        angular_error = tf.acos(dot_product)\n",
    "        angular_error=angular_error*57.296\n",
    "        self.total_error.assign_add(tf.reduce_sum(angular_error))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_error / self.num_samples\n",
    "    def reset_state(self):\n",
    "        self.total_error.assign(0.0)\n",
    "        self.num_samples.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data of new person p02 added to the dataset to be tested using the gazemodel if the calibration is done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p00', 'p01', 'p02']\n"
     ]
    }
   ],
   "source": [
    "face,lefteye,righteye,rotation_matrix,gaze,subject_id,eye_coords=[],[],[],[],[],[],[]\n",
    "data_path='processed_data/Image'\n",
    "persons=os.listdir(data_path)\n",
    "persons.sort()\n",
    "print(persons)\n",
    "for person in persons:\n",
    "    face+=glob.glob(f'{data_path}/{person}/face/*')\n",
    "    lefteye+=glob.glob(f'{data_path}/{person}/lefteye/*')\n",
    "    righteye+=glob.glob(f'{data_path}/{person}/righteye/*')\n",
    "    rotation_matrix+=glob.glob(f'{data_path}/{person}/rotation_matrix/*')\n",
    "    gaze+=glob.glob(f'{data_path}/{person}/3d_gaze/*')\n",
    "    subject_id+=[f'{data_path}/{person}' for _ in range(len(face))]\n",
    "    eye_coords+=glob.glob(f'{data_path}/{person}/eye_coords/*')\n",
    "face.sort()\n",
    "lefteye.sort()\n",
    "righteye.sort()\n",
    "rotation_matrix.sort()\n",
    "gaze.sort()\n",
    "eye_coords.sort()\n",
    "subject_id.sort()\n",
    "data=list(zip(face,lefteye,righteye,rotation_matrix,eye_coords,subject_id,gaze))   \n",
    "random.seed(12)\n",
    "random.shuffle(data)\n",
    "data=tf.data.experimental.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.0818105 ,  0.18091688, -0.9931335 ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model(next(iter(data.map(map_fn_gaze).batch(1).map(lambda x,y:x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking evaluation metrics for the dataset alongwith new person data using the updated gazemodel after calibrating the embeddings using calibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,labels=[],[]\n",
    "for dat,label in data.map(map_fn_gaze).batch(1000):\n",
    "    preds+=gaze_model(dat).numpy().tolist()\n",
    "    labels+=label.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean angular error  4.3756385\n"
     ]
    }
   ],
   "source": [
    "er=AngularError()\n",
    "er.update_state(labels,preds)\n",
    "print('mean angular error ',er.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
